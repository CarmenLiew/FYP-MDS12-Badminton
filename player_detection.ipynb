{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 268 layers, 43,668,288 parameters, 0 gradients, 165.2 GFLOPs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 167\u001b[0m\n\u001b[0;32m    161\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()  \u001b[38;5;66;03m# Display the final frame\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# detector = ObjectDetection(capture=\"test.mp4\", result='result', court=(408, 578, 1500, 1031))\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# detector = ObjectDetection(capture=\"test.mp4\", result='result', court=(1476, 980, 444, 389))\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# detector = ObjectDetection(capture=\"test.mp4\", result='result', court=(389, 444, 1476, 980 ))\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mObjectDetection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcourt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m450\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m390\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m detector()\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mObjectDetection.__init__\u001b[1;34m(self, capture, result, court)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLASS_NAMES_DICT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnames\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer_colors \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# dictionary to store all colours for the players\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m tracknet_ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mtracknet_file)\n\u001b[0;32m     27\u001b[0m tracknet_seq_len \u001b[38;5;241m=\u001b[39m tracknet_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam_dict\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m bg_mode \u001b[38;5;241m=\u001b[39m tracknet_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam_dict\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbg_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# all 4 players with different colours\n",
    "#### Change by using plt instead of cv2.imshow ####\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Set this to an interactive backend suitable for your system\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import cvzone\n",
    "from sort import *\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from utils.general import *\n",
    "\n",
    "class ObjectDetection():\n",
    "\n",
    "    def __init__(self, capture, result, court):\n",
    "        self.capture = capture\n",
    "        self.result = result\n",
    "        self.court = court  # court coordinates (xmin, ymin, xmax, ymax)\n",
    "        self.model = self.load_model()\n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "        self.player_colors = {} # dictionary to store all colours for the players\n",
    "\n",
    "        tracknet_ckpt = torch.load(args.tracknet_file)\n",
    "        tracknet_seq_len = tracknet_ckpt['param_dict']['seq_len']\n",
    "        bg_mode = tracknet_ckpt['param_dict']['bg_mode']\n",
    "        tracknet = get_model('TrackNet', tracknet_seq_len, bg_mode).cuda()\n",
    "        tracknet.load_state_dict(tracknet_ckpt['model'])\n",
    "\n",
    "    def load_model(self):\n",
    "        model = YOLO(\"yolov8l.pt\")\n",
    "        model.fuse()\n",
    "        return model\n",
    "\n",
    "    def predict(self, img):\n",
    "        results = self.model(img, stream=True)\n",
    "        return results\n",
    "\n",
    "    def plot_boxes(self, results, img, detections):\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "\n",
    "                # Classname\n",
    "                cls = int(box.cls[0])\n",
    "                currentClass = self.CLASS_NAMES_DICT[cls]\n",
    "\n",
    "                # Confidence score\n",
    "                conf = math.ceil(box.conf[0] * 100) / 100\n",
    "\n",
    "                # Check if bounding box intersects with the court\n",
    "                if conf > 0.5 and self.is_within_court(x1, y1, x2, y2):\n",
    "                    currentArray = np.array([x1, y1, x2, y2, conf])\n",
    "                    detections = np.vstack((detections, currentArray))\n",
    "                    # Optionally, draw the bounding box\n",
    "                    # cvzone.putTextRect(img, f'class: {currentClass}', (x1, y1), scale=1, thickness=1, colorR=(0, 0, 255))\n",
    "                    # cvzone.cornerRect(img, (x1, y1, w, h), l=9, rt=1, colorR=(255, 0, 255))\n",
    "\n",
    "        return detections, img\n",
    "\n",
    "    def is_within_court(self, x1, y1, x2, y2):\n",
    "        xmin, ymin, xmax, ymax = self.court\n",
    "        # Check if any part of the bounding box is within the court\n",
    "        return not (x2 < xmin or x1 > xmax or y2 < ymin or y1 > ymax)\n",
    "\n",
    "    def track_detect(self, detections, tracker, img):\n",
    "        resultTracker = tracker.update(detections)\n",
    "\n",
    "        # Predefined colors (you can choose any colors you like)\n",
    "        color_list = [\n",
    "            (0, 0, 255),   # Red\n",
    "            (0, 255, 0),   # Green\n",
    "            (255, 0, 0),   # Blue\n",
    "            (255, 255, 0)  # Yellow\n",
    "        ]\n",
    "\n",
    "        for res in resultTracker:\n",
    "            x1, y1, x2, y2, id = res\n",
    "            x1, y1, x2, y2, id = int(x1), int(y1), int(x2), int(y2), int(id)\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            cx, cy = x1 + w // 2, y1 + h // 2\n",
    "\n",
    "            # Assign a color to the player ID if it hasn't been assigned yet\n",
    "            if id not in self.player_colors:\n",
    "                if len(self.player_colors) < len(color_list):\n",
    "                    self.player_colors[id] = color_list[len(self.player_colors)]\n",
    "                else:\n",
    "                    # If more than 4 players, recycle colors (or add more unique colors)\n",
    "                    self.player_colors[id] = color_list[id % len(color_list)]\n",
    "\n",
    "            # Get the assigned color for this player\n",
    "            color = self.player_colors[id]\n",
    "\n",
    "            cvzone.putTextRect(img, f'Player: {id}', (x1, y1), scale=1, thickness=1, colorR=color)\n",
    "            cvzone.cornerRect(img, (x1, y1, w, h), l=9, rt=1, colorR=color)\n",
    "\n",
    "            print(f\"Player ID: {id}, Coordinates: ({cx}, {cy})\")\n",
    "\n",
    "            cv2.circle(img, (cx, cy), 5, color, cv2.FILLED)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __call__(self):\n",
    "        cap = cv2.VideoCapture(self.capture)\n",
    "        assert cap.isOpened(), \"Failed to open video capture\"\n",
    "\n",
    "        result_path = os.path.join(self.result, 'results.avi')\n",
    "\n",
    "        codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        vid_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        vid_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        vid_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        out = cv2.VideoWriter(result_path, codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "        tracker = Sort(max_age=20, min_hits=3, iou_threshold=0.3)\n",
    "\n",
    "        if not os.path.exists(self.result):\n",
    "            os.makedirs(self.result)\n",
    "            print(\"Result folder created successfully\")\n",
    "        else:\n",
    "            print(\"Result folder already exists\")\n",
    "\n",
    "        # Set up the matplotlib figure and axes\n",
    "        plt.ion()  # Interactive mode on\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            assert img.shape[0] == vid_height and img.shape[1] == vid_width, \"Frame dimensions do not match video dimensions\"\n",
    "\n",
    "            detections = np.empty((0, 5))\n",
    "            results = self.predict(img)\n",
    "            detections, frames = self.plot_boxes(results, img, detections)\n",
    "            detect_frame = self.track_detect(detections, tracker, frames)\n",
    "\n",
    "            out.write(detect_frame)\n",
    "\n",
    "            # Convert BGR to RGB for displaying with matplotlib\n",
    "            img_rgb = cv2.cvtColor(detect_frame, cv2.COLOR_BGR2RGB)\n",
    "            ax.clear()\n",
    "            ax.imshow(img_rgb)\n",
    "            plt.draw()\n",
    "            plt.pause(0.001)  # Small pause to allow for image display\n",
    "\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        plt.ioff()  # Turn off interactive mode\n",
    "        plt.show()  # Display the final frame\n",
    "\n",
    "\n",
    "# detector = ObjectDetection(capture=\"test.mp4\", result='result', court=(408, 578, 1500, 1031))\n",
    "# detector = ObjectDetection(capture=\"test.mp4\", result='result', court=(1476, 980, 444, 389))\n",
    "# detector = ObjectDetection(capture=\"test.mp4\", result='result', court=(389, 444, 1476, 980 ))\n",
    "detector = ObjectDetection(capture=\"test.mp4\", result='result', court=(450, 390, 1500, 1000))\n",
    "\n",
    "detector()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
